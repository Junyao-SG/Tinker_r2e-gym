# Tinker GRPO training config for SWE agents
model:
  model_name: "Qwen/Qwen3-30B-A3B"
  lora_rank: 32
  lora_alpha: 64

training:
  learning_rate: 2.0e-5
  num_steps: 1000
  group_size: 10
  batch_size: 8
  temperature: 1.0
  kl_coeff: 0.01
  save_every: 50

rollout:
  dataset: "R2E-Gym/R2E-Gym-Lite"
  split: "train"
  max_steps: 40
  max_workers: 20
  backend: "kubernetes"  # "docker" for local, "kubernetes" for EKS
  scaffold: "r2egym"
  use_fn_calling: false   # Open-weight models use XML tool format

output:
  log_dir: "/data/training/"
